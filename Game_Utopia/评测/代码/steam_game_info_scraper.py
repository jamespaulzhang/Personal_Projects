import requests
import pandas as pd
import re
from time import sleep
import random
from bs4 import BeautifulSoup
import urllib3
import hashlib

# Disable all warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# ç™¾åº¦ç¿»è¯‘APIé…ç½®
BAIDU_APP_ID = '20250530002369816'
BAIDU_SECRET_KEY = 'SuRrNE3RiYhTApBuH4it'

def clean_dataframe(df):
    """Ensure data types are correct"""
    str_columns = ['æ¸¸æˆç±»å‹', 'å•†åº—è¯„ä»·', 'æ¸¸æˆå•†åº—é“¾æ¥', 'æ¸¸æˆå‘è¡Œå•†']
    for col in str_columns:
        if col in df.columns:
            df[col] = df[col].astype(str).replace('nan', '')
    return df

def extract_app_id(url):
    """Improved appid extraction function"""
    if pd.isna(url) or not isinstance(url, str):
        return None
    match = re.search(r'/app/(\d+)', url)
    return match.group(1) if match else None

def translate_with_baidu(text, from_lang='en', to_lang='zh'):
    """ä½¿ç”¨ç™¾åº¦ç¿»è¯‘APIç¿»è¯‘æ–‡æœ¬"""
    if not text.strip():
        return text
    
    salt = random.randint(32768, 65536)
    sign = hashlib.md5((BAIDU_APP_ID + text + str(salt) + BAIDU_SECRET_KEY).encode()).hexdigest()
    
    url = "https://fanyi-api.baidu.com/api/trans/vip/translate"
    params = {
        'q': text,
        'from': from_lang,
        'to': to_lang,
        'appid': BAIDU_APP_ID,
        'salt': salt,
        'sign': sign
    }
    
    try:
        response = requests.get(url, params=params, timeout=10)
        result = response.json()
        if 'trans_result' in result:
            return ' '.join([item['dst'] for item in result['trans_result']])
        return text
    except Exception as e:
        print(f"ç¿»è¯‘å¤±è´¥: {str(e)}")
        return text

def get_adult_cookie(age=21):
    """ç”Ÿæˆæˆäººå†…å®¹éªŒè¯cookie"""
    return {
        'birthtime': '0',
        'mature_content': '1',
        'lastagecheckage': f'1-January-{2023-age}',
        'wants_mature_content': '1',
        'steamAgeVerified': '1'
    }

def get_publisher_info(session, app_id):
    """è·å–æ¸¸æˆå‘è¡Œå•†ä¿¡æ¯"""
    url = f"https://store.steampowered.com/api/appdetails?appids={app_id}&l=schinese"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
    }
    cookies = get_adult_cookie()
    
    try:
        response = session.get(url, headers=headers, cookies=cookies, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        if str(app_id) in data and data[str(app_id)]['success']:
            game_data = data[str(app_id)]['data']
            # ä¼˜å…ˆè·å–å‘è¡Œå•†(publishers)ï¼Œå¦‚æœæ²¡æœ‰åˆ™è·å–å¼€å‘å•†(developers)
            publishers = game_data.get('publishers', [])
            if not publishers:
                publishers = game_data.get('developers', [])
            
            # å¦‚æœæœ‰å¤šä¸ªå‘è¡Œå•†ï¼Œç”¨é€—å·åˆ†éš”
            if publishers:
                return ', '.join(publishers)
        
        return "æœªçŸ¥å‘è¡Œå•†"
    except Exception as e:
        print(f"âŒ è·å–å‘è¡Œå•†ä¿¡æ¯å¤±è´¥: {str(e)[:100]}")
        return "è·å–å¤±è´¥"

def scrape_steam_page(session, app_id):
    """ä»Steamå•†åº—é¡µé¢çˆ¬å–ç”¨æˆ·å®šä¹‰çš„æ ‡ç­¾ - æ”¹è¿›ç‰ˆï¼Œç‰¹åˆ«å¤„ç†æˆäººå†…å®¹"""
    url = f"https://store.steampowered.com/app/{app_id}/"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
        "Accept-Language": "zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7",
    }
    
    # è®¾ç½®æˆäººå†…å®¹cookie
    cookies = get_adult_cookie()
    
    try:
        # ç¬¬ä¸€æ¬¡å°è¯•ï¼šå¸¦æˆäººcookieè¯·æ±‚
        response = session.get(url, headers=headers, cookies=cookies, timeout=30)
        response.raise_for_status()
        
        # æ£€æŸ¥æ˜¯å¦è¢«å¹´é¾„éªŒè¯æ‹¦æˆª
        if "agecheck" in response.url:
            # æäº¤å¹´é¾„éªŒè¯è¡¨å•
            soup = BeautifulSoup(response.text, 'html.parser')
            form = soup.find('form', {'id': 'agecheck_form'})
            if form:
                data = {
                    'ageDay': '1',
                    'ageMonth': 'January',
                    'ageYear': '1980',
                    'snr': '1_agecheck_agecheck__age-gate',
                    'sessionid': session.cookies.get('sessionid', '')
                }
                # è·å–è¡¨å•action URL
                action_url = form.get('action', url)
                if not action_url.startswith('http'):
                    action_url = f"https://store.steampowered.com{action_url}"
                
                # æäº¤å¹´é¾„éªŒè¯
                response = session.post(action_url, data=data, headers=headers, cookies=cookies, timeout=30)
                response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æ–¹æ³•1ï¼šå°è¯•è·å–æµè¡Œæ ‡ç­¾
        tags = []
        tags_section = soup.select('div.glance_tags.popular_tags a')
        for tag in tags_section:
            tag_text = tag.text.strip()
            if tag_text and tag_text not in tags:
                tags.append(tag_text)
        
        # æ–¹æ³•2ï¼šå¦‚æœæ ‡ç­¾å¤ªå°‘ï¼Œå°è¯•ä»è¯¦æƒ…éƒ¨åˆ†è·å–
        if len(tags) < 3:
            details_section = soup.find('div', class_='details_block')
            if details_section:
                for b in details_section.find_all('b'):
                    if 'ç±»å‹' in b.text or 'Genre' in b.text or 'æ ‡ç­¾' in b.text or 'Tag' in b.text:
                        genre_text = b.next_sibling.strip()
                        if genre_text:
                            tags.extend([g.strip() for g in genre_text.split(',') if g.strip()])
        
        # æ–¹æ³•3ï¼šå¦‚æœè¿˜æ˜¯å¤ªå°‘ï¼Œå°è¯•ä»æ¸¸æˆæè¿°ä¸­æå–å…³é”®è¯
        if len(tags) < 3:
            description = soup.find('div', class_='game_description_snippet')
            if description:
                desc_text = description.get_text().strip()
                if desc_text and len(desc_text.split()) > 2:
                    tags.append(desc_text)
        
        # æ–¹æ³•4ï¼šå°è¯•ä»APIè·å–æ›´å¤šæ ‡ç­¾
        if len(tags) < 3:
            api_url = f"https://store.steampowered.com/api/appdetails?appids={app_id}&l=schinese"
            api_response = session.get(api_url, headers=headers, cookies=cookies, timeout=30)
            if api_response.status_code == 200:
                api_data = api_response.json()
                if str(app_id) in api_data and api_data[str(app_id)]['success']:
                    data = api_data[str(app_id)]['data']
                    # è·å–å®˜æ–¹åˆ†ç±»
                    if 'genres' in data:
                        tags.extend([g['description'] for g in data['genres']])
                    # è·å–å¼€å‘å•†/å‘è¡Œå•†ä¿¡æ¯
                    if 'developers' in data:
                        tags.extend(data['developers'])
                    if 'publishers' in data:
                        tags.extend(data['publishers'])
        
        return list(set(tags))[:15]  # é™åˆ¶æœ€å¤š15ä¸ªæ ‡ç­¾é¿å…è¿‡å¤š
    
    except Exception as e:
        print(f"âŒ çˆ¬å–å¤±è´¥: {str(e)[:100]}")
        return []

def get_steam_game_info(session, app_id, api_key):
    """Fetch game info using Steam Web API and scraping - æ”¹è¿›ç‰ˆï¼Œç‰¹åˆ«å¤„ç†æˆäººå†…å®¹"""
    user_tags = scrape_steam_page(session, app_id)
    details_url = f"https://store.steampowered.com/api/appdetails"
    details_params = {"appids": app_id, "l": "schinese"}  # ä¼˜å…ˆè¯·æ±‚ä¸­æ–‡æ•°æ®

    try:
        # è®¾ç½®æˆäººå†…å®¹cookie
        cookies = get_adult_cookie()
        
        details_response = session.get(details_url, params=details_params, cookies=cookies, timeout=30)
        details_response.raise_for_status()
        details_data = details_response.json()

        game_data = details_data.get(str(app_id), {}).get("data", {}) if details_data.get(str(app_id), {}).get("success") else {}
        api_genres = [genre["description"] for genre in game_data.get("genres", [])]

        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–‡æ•°æ®
        has_chinese = 'schinese' in game_data.get('supported_languages', '').lower()
        
        # åˆå¹¶æ‰€æœ‰æ ‡ç­¾
        all_tags = list(set(user_tags + api_genres))
        
        # å¦‚æœæ²¡æœ‰ä¸­æ–‡æ•°æ®ï¼Œåˆ™ç¿»è¯‘è‹±æ–‡æ ‡ç­¾
        if not has_chinese and all_tags:
            translated_tags = []
            for tag in all_tags:
                # ç®€å•æ£€æŸ¥æ˜¯å¦æ˜¯ä¸­æ–‡å­—ç¬¦
                if not any('\u4e00' <= char <= '\u9fff' for char in tag):
                    translated = translate_with_baidu(tag)
                    translated_tags.append(translated)
                else:
                    translated_tags.append(tag)
            all_tags = translated_tags
        
        genres_str = ', '.join(all_tags) if all_tags else 'æœªçŸ¥'

        # è·å–è¯„ä»·ä¿¡æ¯
        reviews_url = f"https://store.steampowered.com/appreviews/{app_id}"
        reviews_params = {
            "json": 1, 
            "key": api_key, 
            "language": "schinese",
            "filter": "all",  # è·å–æ‰€æœ‰è¯„ä»·
            "purchase_type": "all"  # åŒ…æ‹¬éSteamè´­ä¹°çš„è¯„ä»·
        }

        reviews_response = session.get(reviews_url, params=reviews_params, cookies=cookies, timeout=30)
        reviews_response.raise_for_status()
        reviews_data = reviews_response.json()

        review_summary = reviews_data.get("query_summary", {})
        total_positive = review_summary.get('total_positive', 0)
        total_negative = review_summary.get('total_negative', 0)

        total_reviews = total_positive + total_negative
        if total_reviews > 0:
            positive_rate = total_positive / total_reviews
            review_text = f"{positive_rate:.1%} å¥½è¯„ ({total_positive} å¥½è¯„, {total_negative} å·®è¯„)"
        else:
            review_text = "æ— è¯„ä»·"

        # è·å–å‘è¡Œå•†ä¿¡æ¯
        publisher = get_publisher_info(session, app_id)

        return {
            'æ¸¸æˆç±»å‹': genres_str,
            'å•†åº—è¯„ä»·': review_text,
            'æ¸¸æˆå•†åº—é“¾æ¥': f"https://store.steampowered.com/app/{app_id}/",
            'æ¸¸æˆå‘è¡Œå•†': publisher
        }

    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {str(e)[:100]}")
        return {
            'æ¸¸æˆç±»å‹': ', '.join(user_tags) if user_tags else 'æœªçŸ¥',
            'å•†åº—è¯„ä»·': "æ— è¯„ä»·",
            'æ¸¸æˆå•†åº—é“¾æ¥': f"https://store.steampowered.com/app/{app_id}/",
            'æ¸¸æˆå‘è¡Œå•†': "è·å–å¤±è´¥"
        }

# Your Steam Web API Key
api_key = "578A56730A0159A5AF01CEA6B9075902"

# Read and clean data
df = pd.read_excel("è¯„æµ‹æ±‡æ€».xlsx")
df = clean_dataframe(df)

# ç¡®ä¿æœ‰æ¸¸æˆå‘è¡Œå•†åˆ—
if 'æ¸¸æˆå‘è¡Œå•†' not in df.columns:
    df['æ¸¸æˆå‘è¡Œå•†'] = ''

# Use a session to reuse TCP connections
with requests.Session() as session:
    for index, row in df.iterrows():
        need_update = (row['æ¸¸æˆç±»å‹'] in ['', 'æœªçŸ¥', 'nan'] or
                      row['å•†åº—è¯„ä»·'] in ['', 'æ— è¯„ä»·', 'nan'] or
                      row['æ¸¸æˆå‘è¡Œå•†'] in ['', 'nan'])

        if not need_update:
            continue

        print(f"\n=== Processing row {index+1} ===")
        print(f"Game: {row['æµ‹è¯„æ¸¸æˆï¼ˆå•†åº—å…¨åï¼‰']}")

        app_id = extract_app_id(row['æ¸¸æˆå•†åº—é“¾æ¥'])
        if not app_id:
            print("â© No valid appid, skipping")
            continue

        if info := get_steam_game_info(session, app_id, api_key):
            print(f"âœ… Successfully retrieved - Type: {info['æ¸¸æˆç±»å‹']} | Reviews: {info['å•†åº—è¯„ä»·']} | Publisher: {info['æ¸¸æˆå‘è¡Œå•†']}")

            updates = {}
            if row['æ¸¸æˆç±»å‹'] in ['', 'æœªçŸ¥', 'nan']:
                updates['æ¸¸æˆç±»å‹'] = info['æ¸¸æˆç±»å‹']
            if row['å•†åº—è¯„ä»·'] in ['', 'æ— è¯„ä»·', 'nan']:
                updates['å•†åº—è¯„ä»·'] = info['å•†åº—è¯„ä»·']
            if pd.isna(row['æ¸¸æˆå•†åº—é“¾æ¥']) or row['æ¸¸æˆå•†åº—é“¾æ¥'] == '':
                updates['æ¸¸æˆå•†åº—é“¾æ¥'] = info['æ¸¸æˆå•†åº—é“¾æ¥']
            if pd.isna(row['æ¸¸æˆå‘è¡Œå•†']) or row['æ¸¸æˆå‘è¡Œå•†'] in ['', 'nan']:
                updates['æ¸¸æˆå‘è¡Œå•†'] = info['æ¸¸æˆå‘è¡Œå•†']

            for col, value in updates.items():
                df.at[index, col] = value
        else:
            print("âŒ Retrieval failed")

        # å¢åŠ å»¶è¿Ÿä»¥é¿å…è§¦å‘åçˆ¬
        sleep_time = random.uniform(2, 5)
        sleep(sleep_time)

# Save results
output_path = "è¯„æµ‹æ±‡æ€»_å¸¦å‘è¡Œå•†.xlsx"
df.to_excel(output_path, index=False)
print(f"\nğŸ‰ Processing complete! Results saved to: {output_path}")
